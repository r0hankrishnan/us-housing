---
title: "random-forest"
author: "Rohan Krishnan"
date: "2024-07-30"
output: html_document
---
# Random Forest (with tuning)

## Load libraries + functions
```{r, message = FALSE}
source("../scripts/useful-functions.R")
library(tidyverse)
library(randomForest)
library(caret)
library(gridExtra)
```

## Load data + split
```{r}
#Load and preprocess
df <- read.csv("../data/modelling/modelling.csv")
model_df_preprocess(df)

#Split
train_test_split(df, 0.70, 0.30)
```

## Random Forest (untuned)

###Create inital random forest model
```{r}
#Fit model
set.seed(100)
modelForest <- randomForest(clcsHPI ~ ., data = train, ntree = 1000, importance = TRUE)
```

### Look at important variables
```{r}
forestImpVar <- as.data.frame(importance(modelForest))
forestImpVar$varNames <- row.names(forestImpVar)

forestImpPlot <- forestImpVar %>%
  arrange(`%IncMSE`, IncNodePurity) %>%
  mutate(Name = factor(varNames, levels = varNames)) %>%
  ggplot(aes(x = Name, y = `%IncMSE`)) + 
  geom_segment(aes(x = Name, xend = Name, y = 0, yend = `%IncMSE`)) +
  geom_point(aes(size = IncNodePurity), alpha = 0.8) + 
  labs(title = "Untuned RF") + 
  theme_light() + 
  coord_flip() + 
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank(),
  )
```

### Errors
```{r}
#Predict
predForest <- predict(modelForest, test)

#MSE -- 2.273641 -- super low!
mean((predForest - test$clcsHPI)^2)

#RMSE-- 1.50786 -- very similar to MSE
sqrt(mean((predForest - test$clcsHPI)^2))

#MAE -- 1.107139
mean(abs(predForest - test$clcsHPI))

#MAPE -- 0.006166471
mean(abs((predForest - test$clcsHPI)/test$clcsHPI))
```

## Random Forest (tuned)

### Find best tuning parameters using tuneRF
```{r}
# Algorithm Tune (tuneRF)
set.seed(100)
mtryBest <- tuneRF(train[,c(1,2,4:17)], train[,3], stepFactor=1.5, improve=1e-5, ntree=501)
print(mtryBest)
plot(mtryBest) #mtry of 5 seems to work best
```

### Grid search for tuning parameters using caret
```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")

set.seed(100)
tuneGrid <- expand.grid(.mtry=c(1:8))
forestGridSearch <- train(clcsHPI~., data=train, 
                          method="rf", metric="RMSE", 
                          tuneGrid=tuneGrid, trControl=control)
print(forestGridSearch)
plot(forestGridSearch)
```

It looks like tuneRF and our grid search disagree on the optimal mtry. I'll go with an mtry of 8 since it is different than the original, untuned model.  

### Fit tuned random forest
```{r}
#Fit tuned model using tuneRF
set.seed(100)
modelForestTuned <- randomForest(clcsHPI~., data=train, 
                           ntree=1000, mtry=8, importance = TRUE)
```

### Look at important variables
```{r}
forestTunedImpVar <- as.data.frame(importance(modelForestTuned))
forestTunedImpVar$varNames <- row.names(forestTunedImpVar)

forestTunedImpPlot <- forestTunedImpVar %>%
  arrange(`%IncMSE`, IncNodePurity) %>%
  mutate(Name = factor(varNames, levels = varNames)) %>%
  ggplot(aes(x = Name, y = `%IncMSE`)) + 
  geom_segment(aes(x = Name, xend = Name, y = 0, yend = `%IncMSE`)) +
  geom_point(aes(size = IncNodePurity), alpha = 0.8) + 
  labs(title = "Tuned RF") + 
  theme_light() + 
  coord_flip() + 
  theme(
    legend.position = "bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank(),
  )
```

## Errors
```{r}
#Predict
predForestTuned <- predict(modelForestTuned, test)

#MSE -- 2.228529 -- slightly better than untuned
mean((predForestTuned - test$clcsHPI)^2)

#RMSE -- 1.492826
sqrt(mean((predForestTuned - test$clcsHPI)^2))

#MAE -- 1.09951
mean(abs(predForestTuned - test$clcsHPI))

#MAPE -- 0.006213133
mean(abs((predForestTuned - test$clcsHPI)/test$clcsHPI))
```

## Compare variable importance between tuned and untuned forest
```{r}
grid.arrange(forestImpPlot, forestTunedImpPlot, nrow =1)
```

